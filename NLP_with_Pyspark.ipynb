{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, struct, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "n_CPUs = os.cpu_count()\n",
    "\n",
    "from pyspark.sql.functions import udf, expr, concat, col, split, lower\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import array, col, explode, lit, struct\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "spark = (SparkSession         \n",
    "         .builder.master(\"local[\"+str(n_CPUs)+\"]\")\n",
    "         .config(\"spark.driver.cores\", str(n_CPUs))\n",
    "         .config(\"spark.driver.memory\", \"58g\")\n",
    "         .config(\"spark.executor.memory\",\"6g\")\n",
    "         .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "         .config(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "         .config(\"spark.memory.offHeap.size\",\"10g\")\n",
    "         .config(\"spark.python.worker.memory\",\"4g\")\n",
    "         .config(\"spark.driver.maxResultSize\",\"8g\")\n",
    "         .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:2.1.0\")\n",
    "         .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:2.1.0,com.johnsnowlabs.nlp:spark-nlp-ocr_2.11:2.1.0\")\n",
    "         .config(\"spark.jars.repositories\",\"http://repo.spring.io/plugins-release/\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "#open the spark webUI\n",
    "import webbrowser\n",
    "webbrowser.open(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,concat_ws, lower, when\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",str(n_CPUs))\n",
    "\n",
    "All_Failure_Data = spark.read.parquet('Source_Data/All_Failure_Data')\n",
    "# All_Failure_Data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%time\n",
    "if refit_preprocessor != 'n' and refit_preprocessor != 'no':\n",
    "    print('re-fitting nlp preprocessing pipeline.....')\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import RegexTokenizer, NGram\n",
    "    from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "    from sparknlp.annotator import *\n",
    "    from sparknlp.common import *\n",
    "    from sparknlp.base import *\n",
    "\n",
    "    from sparknlp.annotator import Tokenizer as nlp_Tokenizer\n",
    "    f = open(\"user_input_files/token_exceptions.txt\",\"r\",encoding=\"utf8\")\n",
    "    token_exceptions = f.read()\n",
    "    f.close()\n",
    "    token_exceptions = [t for t in token_exceptions.split('\\n') if t]\n",
    "        \n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"orig_value\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    ## Tokenizer: Word tokens ##\n",
    "    # Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs.\n",
    "    #https://nlp.johnsnowlabs.com/components.html#Tokenizer\n",
    "    tokenizer = (Tokenizer() \n",
    "                 .setInputCols([\"document\"]) \n",
    "                 .setOutputCol(\"token\") \n",
    "                 .setExceptions(token_exceptions)\n",
    "                 .setCaseSensitiveExceptions(False)\n",
    "                 .setTargetPattern(\"\\\\w+\")\n",
    "                 #each group in each of the infix patterns below will become its own token. those groups starting with (?:...) are non-capturing and are dropped.  \n",
    "                 .setInfixPatterns([\"([\\\\w]*)([0-9_]*)(?<![\\\\d_])([0-9]{3}_[0-9]{1})(?:[0]{2}[0-9]{1})(?!\\\\d{0,4}_)(?:[0-9_]*)([\\\\w]*)\",#split words and 7 digit error codes ending with three zeros. Truncate zeros\n",
    "                                    \"([\\\\w]*)([0-9_]*)(?<![\\\\d_])([0-9]{3}_[1-9]{2})(?:[0]{1}[0-9]{1})(?!\\\\d{0,4}_)(?:[0-9_]*)([\\\\w]*)\",#split words and 7 digit error codes ending with two zeros. Truncate zeros\n",
    "                                    \"([\\\\w]*)([0-9_]*)(?<![\\\\d_])([0-9]{3}_[1-9]{1,2})(?:[0]{1,2})(?!\\\\d{1,4}|_)(?:[0-9_]*)([\\\\w]*)\", #capture and truncate codes like 242_200 -> 242_2, 357_10 -> 357_1 or 133_680 -> 133_68\n",
    "                                    \"([\\\\w]*)([0-9_]*)(?<![\\\\d_])([0-9]{3}_[0-9]{3})(?!\\\\d{0,4}_)(?:[0-9_]*)([\\\\w]*)\",#split words and 7 digit error codes. truncate the error code to 6 digits. \n",
    "                                    \"(\\\\b[A-Za-z]{1}[0-9]{1,6}\\\\b)\",#error codes like c123, r56, etc. \n",
    "                                    \"([A-Za-z]*)([0-9_]{2,20})([A-Za-z]*)\", #split general word and number combinations\n",
    "#                                     \"([\\\\$#]?\\\\d+(?:[^\\\\s\\\\d]{1}\\\\d+)*)\", # Money, Phone number and dates -> http://rubular.com/r/ihCCgJiX4e\n",
    "#                                     \"((?:\\\\p{L}\\\\.)+)\", # Abbreviations -> http://rubular.com/r/nMf3n0axfQ\n",
    "#                                     \"(\\\\p{L}+)(n't\\\\b)\", # Weren't -> http://rubular.com/r/coeYJFt8eM\n",
    "#                                     \"(\\\\p{L}+)('{1}\\\\p{L}+)\", # I'll -> http://rubular.com/r/N84PYwYjQp\n",
    "#                                     \"((?:\\\\p{L}+[^\\\\s\\\\p{L}]{1})+\\\\p{L}+)\", # foo-bar -> http://rubular.com/r/cjit4R6uWd\n",
    "#                                     \"([\\\\p{L}\\\\w]+)\" # basic word token\n",
    "                                   ]) \n",
    "                )\n",
    "\n",
    "    ## Normalizer: Text cleaning ##\n",
    "    # Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n",
    "    #https://nlp.johnsnowlabs.com/components.html#Normalizer\n",
    "    normalizer = (Normalizer() \n",
    "                  .setInputCols([\"token\"]) \n",
    "                  .setOutputCol(\"normalized\")\n",
    "                  .setLowercase(True)\n",
    "                  ._set(cleanupPatterns=['[^A-Za-z0-9\\\\\\/\\-_:]']) #Regular expressions list for normalization, defaults [^A-Za-z])\n",
    "    #               .setCleanupPatterns()  #BUG!!! - This functionality does not work. Workaround is to use '_set(cleanupPatterns=\"[^A-Za-z]\")'\n",
    "                  .setSlangDictionary('user_input_files/BD_Slang_Dictionary.csv', delimiter = ',')#: path to custom word mapping for spell checking. e.g. gr8 -> great. Uses provided delimiter, readAs LINE_BY_LINE or SPARK_DATASET with options passed to reader if the latter.\n",
    "                 )\n",
    "\n",
    "\n",
    "\n",
    "#     #########################################\n",
    "#     ### Build Autocorrect Training Corpus ####\n",
    "#     ### In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. ###\n",
    "\n",
    "#     from sparknlp.ocr import OcrHelper\n",
    "#     from pyspark.sql.functions import rand\n",
    "#     ocr = OcrHelper()\n",
    "#     spell_data_ocr = ocr.createDataset(spark, \"pdf_text_sources/\").withColumnRenamed(\"value\",\"text\")\n",
    "\n",
    "#     #use the brown corpus of scientific texts to train the autocorrect\n",
    "#     from nltk.corpus import brown\n",
    "#     spell_data_brown = spark.createDataFrame([' '.join(i) for i in brown.sents(categories=['learned','government','news','hobbies'])],StringType()).withColumnRenamed(\"value\",\"text\")\n",
    "\n",
    "#     #training set of the top 500 uncorrected words from the last model run. Each row has all 500 words. There are n number of rows defined below\n",
    "#     spell_data_seed_top_BD_words = spark.read.text('misc_input_files/autocorrect_train_seed_top_BD_words.txt').withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "#     # combine the lvp text, brown_corpus, and  data into our training set. \n",
    "#     spell_data = spell_data_ocr.select('text').union(spell_data_brown).union(spell_data_seed_top_BD_words).orderBy(rand())\n",
    "\n",
    "#     documentAssembler_spl_chk = DocumentAssembler()\\\n",
    "#       .setInputCol(\"text\")\\\n",
    "#       .setOutputCol(\"document\")\n",
    "\n",
    "\n",
    "#     spell_check_training_data_pipeline = Pipeline(stages=[documentAssembler_spl_chk,tokenizer,normalizer])\n",
    "#     spell_check_train_fit = spell_check_training_data_pipeline.fit(spell_data)\n",
    "#     spell_check_train_data = (spell_check_train_fit\n",
    "#                               .transform(spell_data)\n",
    "#                               .select('normalized')\n",
    "#                              )\n",
    "# #     spell_check_train_data.write.parquet(\"misc_input_files/spell_check_train_data\", mode = \"overwrite\")\n",
    "# #     spell_check_train_data.show(10,truncate=150)\n",
    "\n",
    "#     ### End Build Autocorrect Training Corpus ####\n",
    "#     ##############################################\n",
    "    \n",
    "\n",
    "    from pyspark.ml.feature import SQLTransformer\n",
    "    from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "    # SpellChecker: Norvig algorithm ##\n",
    "    # This annotator retrieves tokens and makes corrections automatically if not found on an english dictionary\n",
    "    # https://nlp.johnsnowlabs.com/components.html#SpellChecker\n",
    "    spell_check_train_data = spark.read.parquet(\"misc_input_files/spell_check_train_data\")\n",
    "    spell_checker = (NorvigSweetingApproach() \n",
    "                     .setInputCols([\"normalized\"]) \n",
    "                     .setOutputCol(\"autocorrected\")\n",
    "                     .setDictionary('misc_input_files/BD_Dictionary.txt',token_pattern=\"\\w+\", read_as=ReadAs.LINE_BY_LINE)\n",
    "#                      .setFrequencyPriority(True)\n",
    "                     .fit(spell_check_train_data)\n",
    "                    )\n",
    "\n",
    "\n",
    "    # # Lemmatizer: Lemmas ##\n",
    "    # Retrieves lemmas out of words with the objective of returning a base dictionary word\n",
    "    # https://nlp.johnsnowlabs.com/components.html#Lemmatizer\n",
    "    # from sparknlp.annotator import LemmatizerModel\n",
    "    lemmatizer = (Lemmatizer()\n",
    "    #               .pretrained()\n",
    "                  .setInputCols([\"autocorrected\"]) \n",
    "                  .setOutputCol(\"lemmas\")\n",
    "                  .setDictionary(\"user_input_files/BD_lemmas.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\"))\n",
    "\n",
    "    custom_stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours'\n",
    "                 , 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they'\n",
    "                 , 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are'\n",
    "                 , 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if'\n",
    "                 , 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during'\n",
    "                 , 'before', 'after', 'to', 'from', 'up', 'down', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then'\n",
    "                 , 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such'\n",
    "                 , 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\"\n",
    "                 , 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn'\n",
    "                 , \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\"\n",
    "                 , 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "                 , \"nan\",\"na\",'xxx','due', \"dont\", \"shouldve\", \"arent\", \"couldnt\", \"didnt\", \"doesnt\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"shouldnt\", \"wasnt\", \"werent\",\"wont\", \"wouldnt\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n",
    "\n",
    "    # stop_words_remover = StopWordsRemover(inputCol=\"lemmas\", outputCol=\"lemmas_no_sw\",stopWords=custom_stop_words)\n",
    "    stop_words_remover = (StopWordsCleaner() \n",
    "            .setInputCols([\"lemmas\"]) \n",
    "            .setOutputCol(\"lemmas_no_sw\") \n",
    "            .setCaseSensitive(False) \n",
    "            .setStopWords(custom_stop_words)\n",
    "                         )\n",
    "\n",
    "\n",
    "    # Stemmer: Hard stems ##\n",
    "    #Returns hard-stems out of words with the objective of retrieving the meaningful part of the word\n",
    "    #https://nlp.johnsnowlabs.com/components.html#Stemmer\n",
    "    stemmer = (Stemmer() \n",
    "      .setInputCols(\"lemmas_no_sw\") \n",
    "      .setOutputCol(\"stem_no_sw\")\n",
    "              )\n",
    "\n",
    "\n",
    "    #finisher to remove meta data \n",
    "    finisher = Finisher() \\\n",
    "        .setInputCols([\"token\",\"normalized\",\"autocorrected\",\"lemmas\",\"lemmas_no_sw\",\"stem_no_sw\"]) \\\n",
    "        .setOutputCols([\"token\",\"normalized\",\"autocorrected\",\"lemmas\",\"lemmas_no_sw\",\"stem_no_sw\"]) \\\n",
    "        .setIncludeMetadata(False)\n",
    "\n",
    "\n",
    "    #this part is really slow for some reason. would it be faster if we did it on the un-finished cols?\n",
    "    bigrams = NGram(n=2, inputCol=\"lemmas\", outputCol=\"bigrams\")\n",
    "    trigrams = NGram(n=3, inputCol=\"lemmas\", outputCol=\"trigrams\")\n",
    "\n",
    "    ml_bigrams = NGram(n=2, inputCol=\"stem_no_sw\", outputCol=\"ml_bigrams\")\n",
    "\n",
    "    #concatenate the token columns for tfidf. \n",
    "    token_and_ngram_assembler = SQLTransformer(statement=\"SELECT *, concat(stem_no_sw,ml_bigrams) as all_tokens_and_ngrams FROM __THIS__\")\n",
    "\n",
    "    #set sequence of preprocessing pipeline\n",
    "    preprocess_pipeline = Pipeline(stages=[documentAssembler,tokenizer,normalizer,spell_checker,lemmatizer,stop_words_remover,stemmer,finisher,bigrams,trigrams,ml_bigrams,token_and_ngram_assembler])\n",
    "\n",
    "#     Fit the pipeline to training documents.\n",
    "    preprocess_pipeline_fit = preprocess_pipeline.fit(distinct_texts_for_preprocessing)\n",
    "    preprocess_pipeline_fit.write().overwrite().save(os.getcwd()+\"/model_data/inputs/preprocess_pipeline/fitted\")\n",
    "else:\n",
    "    preprocess_pipeline_fit = PipelineModel.load(\"file:///\"+os.getcwd()+\"/model_data/inputs/preprocess_pipeline/fitted\")\n",
    "    \n",
    "tokenized_model_input = preprocess_pipeline_fit.transform(distinct_texts_for_preprocessing)\n",
    "\n",
    "tokenized_model_input.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts of how many unique n-grams there are for each field\n",
    "This will give us an idea of scale and where we can reduce complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from pyspark.sql.functions import count, max,when, udf\n",
    "from pyspark.sql.types import *\n",
    "create_spark_model_vec_start_time = time.time()\n",
    "\n",
    "null_array = udf(lambda z: [\"blank\"], ArrayType(StringType()))\n",
    "\n",
    "\n",
    "min_token_count = 3\n",
    "\n",
    "dict_n_features = {}\n",
    "min_token_count_dict = {'Failure_all_ngrams': 2, 'FailureDescription_all_ngrams': 3, 'ObservedSymptom_all_ngrams': 3, 'Operation_Short_Text_all_ngrams': 3, 'Service_Order_Text_all_ngrams': 5, 'parts_list': 3, 'ReferenceDesignator_all_ngrams': 3\n",
    "                        , 'Other_Subsystem_Keys':3,'RootCause_all_ngrams':5, 'Investigation_Conclusion_all_ngrams':5, 'Investigation_Summary_all_ngrams':5\n",
    "                        ,'Concat_Complaint_Text_all_ngrams':5,'Concat_Failure_Text_all_ngrams':2}\n",
    "\n",
    "max_token_pct_dict = {'Failure_all_ngrams': .05, 'FailureDescription_all_ngrams': .05, 'ObservedSymptom_all_ngrams': .05, 'Operation_Short_Text_all_ngrams': .05, 'Service_Order_Text_all_ngrams': .05\n",
    "                        , 'parts_list': .25, 'ReferenceDesignator_all_ngrams': .25, 'Other_Subsystem_Keys':.8, 'RootCause_all_ngrams':1, 'Investigation_Conclusion_all_ngrams':1\n",
    "                      , 'Investigation_Summary_all_ngrams':1,'Concat_Complaint_Text_all_ngrams':.15,'Concat_Failure_Text_all_ngrams':.25}\n",
    "\n",
    "\n",
    "text_cols_for_distinct_counts = ['parts_list','ReferenceDesignator_all_ngrams','Concat_Complaint_Text_all_ngrams','Concat_Failure_Text_all_ngrams','Other_Subsystem_Keys']\n",
    "\n",
    "\n",
    "for column_name in text_cols_for_distinct_counts:\n",
    "    n_features = model_input_df.selectExpr(f\"explode({column_name}) as value\").groupBy('value').count().filter(f\"count >={min_token_count_dict[column_name]} and count <= {num_failures*max_token_pct_dict[column_name]}\").count()\n",
    "    dict_n_features.update({column_name : n_features})\n",
    "\n",
    "\n",
    "id_cols_for_distinct_counts = ['System_key','Subsystem_key']\n",
    "for column_name in id_cols_for_distinct_counts:\n",
    "    n_features = (model_input_df\n",
    "                  .selectExpr(f\"{column_name} as x\")\n",
    "                  .withColumn('x', col('x').cast(IntegerType()))\n",
    "                  .agg({\"x\": \"max\"})\n",
    "                  .collect()[0][\"max(x)\"])\n",
    "    dict_n_features.update({column_name : n_features})\n",
    "    \n",
    "print(dict_n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin vectorization of data\n",
    "TFIDF and OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import array, col, explode, lit, struct\n",
    "from pyspark.ml.feature import HashingTF, IDF,VectorAssembler,OneHotEncoderEstimator, Normalizer, PCA,StandardScaler,CountVectorizer\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "# text_cols = ['Failure', 'FailureDescription','ObservedSymptom','Operation_Short_Text','Service_Order_Text','parts_list','ReferenceDesignator']\n",
    "text_cols = ['parts_list','ReferenceDesignator_all_ngrams','Concat_Complaint_Text_all_ngrams','Concat_Failure_Text_all_ngrams','Other_Subsystem_Keys']\n",
    "\n",
    "n_features = sum([dict_n_features[colname] for colname in text_cols+id_cols_for_distinct_counts])\n",
    "n_classes = model_input_df.selectExpr(\"Resultant_Category as value\").filter('value is not null').groupBy('value').count().count()\n",
    "print(n_features)\n",
    "print(n_classes)\n",
    "\n",
    "vectorized_model_input_df = model_input_df\n",
    "# vectorized_model_input_df.cache()\n",
    "\n",
    "# for column in text_cols:\n",
    "#     tf = HashingTF(numFeatures=dict_n_features[column], inputCol=column, outputCol=column+\"_TFOut\")#.setInputCol(column).setOutputCol(column+\"_TFOut\")\n",
    "#     idf = IDF(minDocFreq = min_token_count_dict[column], inputCol = column+\"_TFOut\",outputCol = column+\"_TFIDF\")#.setInputCol(column+\"_TFOut\").setOutputCol(column+\"_IDFOut\")\n",
    "#     tf_fit = tf.transform(vectorized_model_input_df)\n",
    "#     vectorized_model_input_df = idf.fit(tf_fit).transform(tf_fit).drop(column+\"_TFOut\")\n",
    "\n",
    "# max_pct_documents = .25\n",
    "\n",
    "for column in text_cols:    \n",
    "    # fit a CountVectorizerModel from the corpus.\n",
    "    cv = CountVectorizer(inputCol=column, outputCol=column+\"_CV\", vocabSize=dict_n_features[column], minDF=min_token_count_dict[column],maxDF=max_token_pct_dict[column], binary = False)\n",
    "    vectorized_model_input_df = cv.fit(vectorized_model_input_df).transform(vectorized_model_input_df)\n",
    "    \n",
    "# vectorized_model_input_df.select(*[column+\"_TFIDF\" for column in text_cols]).show(100,truncate = 20)\n",
    "encoder = OneHotEncoderEstimator(inputCols=['System_key','Subsystem_key'],\n",
    "                                 outputCols=[\"system_vector\", \"subsystem_vector\"])\n",
    "\n",
    "vectorized_model_input_df = (vectorized_model_input_df\n",
    "                             .withColumn('System_key', col('System_key').cast(IntegerType()))\n",
    "                             .withColumn('Subsystem_key', col('Subsystem_key').cast(IntegerType()))\n",
    "                          )\n",
    "vectorized_model_input_df = encoder.fit(vectorized_model_input_df).transform(vectorized_model_input_df)\n",
    "\n",
    "# vectorized_model_input_df.cache()\n",
    "# vectorized_model_input_df.count()\n",
    "# vectorized_model_input_df.write.parquet(\"model_data/inputs/vectorized_model_input_df\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to vectors via Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "vectorized_model_input_df.cache()\n",
    "\n",
    "complaint_w2v = Word2Vec(vectorSize=200, minCount=6, numPartitions=32, inputCol=\"Concat_Complaint_Text_unigrams\", outputCol=\"Complaint_doc_vec\", windowSize=5, seed = 1)\n",
    "failure_w2v = Word2Vec(vectorSize=100, minCount=3, numPartitions=32, inputCol=\"Concat_Failure_Text_unigrams\", outputCol=\"Failure_doc_vec\", windowSize=3, seed = 1)\n",
    "\n",
    "mdl_complaint_w2v = complaint_w2v.fit(vectorized_model_input_df)\n",
    "\n",
    "vectorized_model_input_df = mdl_complaint_w2v.transform(vectorized_model_input_df)\n",
    "vectorized_model_input_df.cache()\n",
    "\n",
    "mdl_failure_w2v = failure_w2v.fit(vectorized_model_input_df)\n",
    "\n",
    "vectorized_model_input_df = mdl_failure_w2v.transform(vectorized_model_input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all the vectors into one for each failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from pyspark.ml.feature import HashingTF, IDF,VectorAssembler,OneHotEncoderEstimator, Normalizer,StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[column+\"_CV\" for column in text_cols]+[\"system_vector\", \"subsystem_vector\"] + ['Complaint_doc_vec','Failure_doc_vec'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\")\n",
    "\n",
    "vectorized_model_input_df = assembler.transform(vectorized_model_input_df)\n",
    "vectorized_model_input_df = normalizer.transform(vectorized_model_input_df)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Resultant_Category\", outputCol=\"label\",handleInvalid =\"keep\",stringOrderType = 'alphabetAsc')\n",
    "\n",
    "vectorized_model_input_df = indexer.fit(vectorized_model_input_df).transform(vectorized_model_input_df\n",
    "                                                                            )\n",
    "vectorized_model_input_df.cache()\n",
    "vectorized_model_input_df.count()\n",
    "\n",
    "vectorized_model_input_df.limit(10).toPandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
